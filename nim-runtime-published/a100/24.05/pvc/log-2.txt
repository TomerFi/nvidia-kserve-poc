===========================================
== NVIDIA Inference Microservice LLM NIM ==
===========================================

NVIDIA Inference Microservice LLM NIM Version 24.05
Model: "mphexwv2ysej/meta-llama3-8b-instruct"

Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
This NIM container is governed by the NVIDIA AI Product Agreement here:
https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/

A copy of this license can be found under /opt/nvidia/LICENSE

2024-06-07 16:57:44,654 [INFO] PyTorch version 2.2.2 available.
2024-06-07 16:57:45,293 [WARNING] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: error
[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024052300
2024-06-07 16:57:45,331 [INFO] [TRT-LLM] [I] TensorRT-LLM inited.
INFO 06-07 16:57:46.189 api_server.py:409] NIM LLM API version 0.1.0
INFO 06-07 16:57:46.191 ngc_profile.py:197] Detected 1 compatible profile(s).
INFO 06-07 16:57:46.191 ngc_injector.py:118] Valid profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1) on GPUs [0]
INFO 06-07 16:57:46.191 ngc_injector.py:123] Selected profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1)
INFO 06-07 16:57:46.884 ngc_injector.py:128] Profile metadata: feat_lora: false
INFO 06-07 16:57:46.884 ngc_injector.py:128] Profile metadata: llm_engine: vllm
INFO 06-07 16:57:46.884 ngc_injector.py:128] Profile metadata: tp: 1
INFO 06-07 16:57:46.884 ngc_injector.py:128] Profile metadata: precision: fp16
INFO 06-07 16:58:57 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/tmp/meta-llama3-8b-instruct-hke2wd3s', speculative_config=None, tokenizer='/tmp/meta-llama3-8b-instruct-hke2wd3s', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO 06-07 16:58:58 utils.py:609] Found nccl from library /usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2
INFO 06-07 16:58:59 selector.py:28] Using FlashAttention backend.
INFO 06-07 16:59:04 model_runner.py:173] Loading model weights took 14.9595 GB
INFO 06-07 16:59:05 gpu_executor.py:119] # GPU blocks: 9473, # CPU blocks: 2048
INFO 06-07 16:59:07 model_runner.py:973] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-07 16:59:07 model_runner.py:977] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-07 16:59:13 model_runner.py:1054] Graph capturing finished in 6 secs.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO 06-07 16:59:13 serving_chat.py:347] Using default chat template:
INFO 06-07 16:59:13 serving_chat.py:347] {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>
INFO 06-07 16:59:13 serving_chat.py:347] 
INFO 06-07 16:59:13 serving_chat.py:347] '+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>
INFO 06-07 16:59:13 serving_chat.py:347] 
INFO 06-07 16:59:13 serving_chat.py:347] ' }}{% endif %}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO 06-07 16:59:13.718 server.py:82] Started server process [33]
INFO 06-07 16:59:13.719 on.py:48] Waiting for application startup.
INFO 06-07 16:59:13.720 on.py:62] Application startup complete.
INFO 06-07 16:59:13.722 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)