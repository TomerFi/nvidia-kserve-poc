# Use the latest image from NVIDIA
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    autoscaling.knative.dev/target: "1" # auto-scaling annotation  name: is-nim
  namespace: kserve-test
  name: is-nim
spec:
  predictor:
    imagePullSecrets: 
      - name: registry-secret
    tolerations:
      - key:  odh-notebook
        value: 'true'
        effect: NoSchedule
    volumes:
      - name: model-store
        emptyDir: {}
      - name: cache
        emptyDir: {}
    serviceAccountName: sa-s3
    containers:
    - command: ["/bin/sh", "-c"]
      args:
        - |
          ln -s /mnt/models/* /model-store/; \
          nemollm_inference_ms \
            --num_gpus=1 \
            --model=llama-2-7b-chat \
            --openai_port=8080 \
            --health_port=8088
      env:
        - name: STORAGE_URI
          value: s3://models/model-store
      name: kserve-container
      image: nvcr.io/ohlfw0olaadg/ea-participants/nemollm-inference-ms:24.01
      imagePullPolicy: IfNotPresent
      ports:
      - containerPort: 8080
        protocol: TCP
      resources:
        limits:
          cpu: "12"
          memory: 256Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "12"
          memory: 128Gi
          nvidia.com/gpu: "1"
      volumeMounts:
        - name: model-store
          readOnly: false
          mountPath: /model-store
        - name: cache
          readOnly: false
          mountPath: /.cache/huggingface/hub
