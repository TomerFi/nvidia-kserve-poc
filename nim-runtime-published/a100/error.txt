
==================================
== Triton Inference Server Base ==
==================================

NVIDIA Release 23.12 (build 77457706)

Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field "model_id" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
[2024-04-19 00:26:15 +0000] [162] [INFO] Starting gunicorn 21.2.0
[2024-04-19 00:26:15 +0000] [162] [INFO] Listening at: http://0.0.0.0:8004 (162)
[2024-04-19 00:26:15 +0000] [162] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2024-04-19 00:26:15 +0000] [163] [INFO] Starting gunicorn 21.2.0
[2024-04-19 00:26:15 +0000] [164] [INFO] Booting worker with pid: 164
[2024-04-19 00:26:15 +0000] [163] [INFO] Listening at: http://0.0.0.0:8080 (163)
[2024-04-19 00:26:15 +0000] [163] [INFO] Using worker: uvicorn.workers.UvicornWorker
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:26:15,888888", "message": "Using selector: EpollSelector"}
{"level": "DEBUG", "file_path": "/usr/local/lib/python3.10/dist-packages/inferencemodeltoolkit/clients/triton/client.py", "line_number": 275, "request_id": "None", "time": "2024-04-19 00:26:15,889889", "message": "Connecting to grpc://0.0.0.0:8001"}
[2024-04-19 00:26:15 +0000] [165] [INFO] Starting gunicorn 21.2.0
[2024-04-19 00:26:15 +0000] [166] [INFO] Booting worker with pid: 166
[2024-04-19 00:26:15 +0000] [165] [ERROR] Connection in use: ('0.0.0.0', 8012)
[2024-04-19 00:26:15 +0000] [165] [ERROR] Retrying in 1 second.
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:26:15,895895", "message": "Using selector: EpollSelector"}
{"level": "DEBUG", "file_path": "/usr/local/lib/python3.10/dist-packages/inferencemodeltoolkit/clients/triton/client.py", "line_number": 275, "request_id": "None", "time": "2024-04-19 00:26:15,896896", "message": "Connecting to grpc://0.0.0.0:8001"}
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:26:15,899899", "message": "Using selector: EpollSelector"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:15,900900", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:26:15,908908", "message": "Using selector: EpollSelector"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:15,910910", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
[TensorRT-LLM][WARNING] gpu_device_ids is not specified, will be automatically set
[TensorRT-LLM][WARNING] max_beam_width is not specified, will use default value of 1
[TensorRT-LLM][WARNING] max_tokens_in_paged_kv_cache is not specified, will use default value
[TensorRT-LLM][WARNING] batch_scheduler_policy parameter was not found or is invalid (must be max_utilization or guaranteed_no_evict)
[TensorRT-LLM][WARNING] enable_chunked_context is not specified, will be set to false.
[TensorRT-LLM][WARNING] kv_cache_free_gpu_mem_fraction is not specified, will use default value of 0.9 or max_tokens_in_paged_kv_cache
[TensorRT-LLM][WARNING] exclude_input_in_output is not specified, will be set to false
[TensorRT-LLM][WARNING] max_attention_window_size is not specified, will use default value (i.e. max_sequence_length)
[TensorRT-LLM][WARNING] enable_kv_cache_reuse is not specified, will be set to false
[TensorRT-LLM][WARNING] Parameter version cannot be read from json:
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'version' not found
[TensorRT-LLM][INFO] No engine version found in the config file, assuming engine(s) built by old builder API.
[TensorRT-LLM][WARNING] Parameter head_size cannot be read from json:
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'head_size' not found
[TensorRT-LLM][WARNING] Parameter max_lora_rank cannot be read from json:
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_lora_rank' not found
[TensorRT-LLM][WARNING] Parameter max_draft_len cannot be read from json:
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] lora_plugin enabled, but no lora module enabled: setting useLoraPlugin to false
[TensorRT-LLM][INFO] Initializing MPI with thread mode 1
[TensorRT-LLM][INFO] MPI size: 1, rank: 0
[TensorRT-LLM][INFO] Rank 0 is using GPU 0
[2024-04-19 00:26:16 +0000] [165] [ERROR] Connection in use: ('0.0.0.0', 8012)
[2024-04-19 00:26:16 +0000] [165] [ERROR] Retrying in 1 second.
[2024-04-19 00:26:17 +0000] [165] [ERROR] Connection in use: ('0.0.0.0', 8012)
[2024-04-19 00:26:17 +0000] [165] [ERROR] Retrying in 1 second.
[2024-04-19 00:26:18 +0000] [165] [ERROR] Connection in use: ('0.0.0.0', 8012)
[2024-04-19 00:26:18 +0000] [165] [ERROR] Retrying in 1 second.
[2024-04-19 00:26:19 +0000] [165] [ERROR] Connection in use: ('0.0.0.0', 8012)
[2024-04-19 00:26:19 +0000] [165] [ERROR] Retrying in 1 second.
[2024-04-19 00:26:20 +0000] [165] [ERROR] Can't connect to ('0.0.0.0', 8012)
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:25,911911", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:25,920920", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:35,922922", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:35,931931", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:45,933933", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:45,937937", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:55,944944", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:26:55,947947", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:05,954954", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:05,958958", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:15,965965", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:15,969969", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:25,976976", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:25,980980", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:35,982982", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:35,991991", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:45,993993", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:46,003003", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 64
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 64
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 8192
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0
[TensorRT-LLM][INFO] Loaded engine size: 12855 MiB
[TensorRT-LLM][WARNING] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:55,995995", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:27:56,014014", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 12901, GPU 13361 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 12903, GPU 13371 (MiB)
[TensorRT-LLM][WARNING] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12852, now: CPU 0, GPU 12852 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 12936, GPU 20249 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 12936, GPU 20257 (MiB)
[TensorRT-LLM][WARNING] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12852 (MiB)
[TensorRT-LLM][INFO] Allocate 18857590784 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 35968 total tokens in paged KV cache, and 64 blocks per sequence
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:28:06,006006", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/engines/async_streaming_triton_grpc.py", "line_number": 99, "request_id": "None", "time": "2024-04-19 00:28:06,024024", "message": "Waiting for model `llama-2-7b-chat` to load : [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:0.0.0.0:8001: Failed to connect to remote host: Connection refused"}
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:28:16,026026", "message": "Using selector: EpollSelector"}
[2024-04-19 00:28:16 +0000] [164] [INFO] Started server process [164]
[2024-04-19 00:28:16 +0000] [164] [INFO] Waiting for application startup.
[2024-04-19 00:28:16 +0000] [164] [INFO] Application startup complete.
{"level": "DEBUG", "file_path": "/usr/lib/python3.10/asyncio/selector_events.py", "line_number": 54, "request_id": "None", "time": "2024-04-19 00:28:16,044044", "message": "Using selector: EpollSelector"}
[2024-04-19 00:28:16 +0000] [166] [INFO] Started server process [166]
[2024-04-19 00:28:16 +0000] [166] [INFO] Waiting for application startup.
[2024-04-19 00:28:16 +0000] [166] [INFO] Application startup complete.
/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:
  Expected `str` but got `bool` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 119, "request_id": "None", "time": "2024-04-19 00:31:55,774774", "message": "Received OpenAI API completion request"}
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/api/interfaces/entrypoints.py", "line_number": 98, "request_id": "None", "time": "2024-04-19 00:31:55,774774", "message": "both temperature and top_p specified"}
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 177, "request_id": "None", "time": "2024-04-19 00:31:55,774774", "message": "Invalid_request_error: both temperature and top_p specified"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 119, "request_id": "None", "time": "2024-04-19 00:32:05,368368", "message": "Received OpenAI API completion request"}
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/api/interfaces/entrypoints.py", "line_number": 98, "request_id": "None", "time": "2024-04-19 00:32:05,369369", "message": "both temperature and top_p specified"}
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 177, "request_id": "None", "time": "2024-04-19 00:32:05,369369", "message": "Invalid_request_error: both temperature and top_p specified"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 197, "request_id": "None", "time": "2024-04-19 10:11:13,427427", "message": "Received chat completion request"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/triton/llm_pipeline/trt_pipeline.py", "line_number": 246, "request_id": "1ddbaf79-8aa8-4b70-a14f-902b0c11b6cb", "time": "2024-04-19 10:11:13,488488", "message": "trt_llm_input_tensors ['input_ids', 'input_lengths', 'request_output_len', 'runtime_top_k', 'runtime_top_p', 'temperature', 'len_penalty', 'repetition_penalty', 'random_seed', 'return_log_probs', 'beam_width', 'end_id', 'stop_words_list', 'bad_words_list']"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/triton/llm_pipeline/trt_pipeline.py", "line_number": 274, "request_id": "1ddbaf79-8aa8-4b70-a14f-902b0c11b6cb", "time": "2024-04-19 10:11:13,488488", "message": "output_names ['output_ids', 'sequence_length', 'output_log_probs', 'cum_log_probs']"}
[TensorRT-LLM][ERROR] Encountered an error in forward function: Input tensor 'attn_qkv_lora_weights_pointers_0' not found; expected shape: (-1, 2) (/app/tensorrt_llm/cpp/tensorrt_llm/runtime/tllmRuntime.cpp:123)
1       0x7f12aa10eb9f tensorrt_llm::runtime::TllmRuntime::setInputTensors(int, std::unordered_map<std::string, std::shared_ptr<tensorrt_llm::runtime::ITensor>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::shared_ptr<tensorrt_llm::runtime::ITensor> > > > const&) + 607
2       0x7f12aa1465de tensorrt_llm::batch_manager::TrtGptModelInflightBatching::setupContext(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > > const&, std::vector<unsigned long, std::allocator<unsigned long> > const&, int, int) + 206
3       0x7f12aa1476f8 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::executeBatch(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > >&) + 1160
4       0x7f12aa14d724 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::forward(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&) + 3716
5       0x7f12aa11da68 tensorrt_llm::batch_manager::GptManager::step(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&) + 56
6       0x7f12aa1227c7 tensorrt_llm::batch_manager::GptManager::decoupled_execution_loop() + 247
7       0x7f137e84f253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f137e84f253]
8       0x7f137e5deac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f137e5deac3]
9       0x7f137e670850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f137e670850]
[TensorRT-LLM][ERROR] Encountered error for requestId 5039179871698361612: Encountered an error in forward function: Input tensor 'attn_qkv_lora_weights_pointers_0' not found; expected shape: (-1, 2) (/app/tensorrt_llm/cpp/tensorrt_llm/runtime/tllmRuntime.cpp:123)
1       0x7f12aa10eb9f tensorrt_llm::runtime::TllmRuntime::setInputTensors(int, std::unordered_map<std::string, std::shared_ptr<tensorrt_llm::runtime::ITensor>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::shared_ptr<tensorrt_llm::runtime::ITensor> > > > const&) + 607
2       0x7f12aa1465de tensorrt_llm::batch_manager::TrtGptModelInflightBatching::setupContext(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > > const&, std::vector<unsigned long, std::allocator<unsigned long> > const&, int, int) + 206
3       0x7f12aa1476f8 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::executeBatch(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > >&) + 1160
4       0x7f12aa14d724 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::forward(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&) + 3716
5       0x7f12aa11da68 tensorrt_llm::batch_manager::GptManager::step(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&) + 56
6       0x7f12aa1227c7 tensorrt_llm::batch_manager::GptManager::decoupled_execution_loop() + 247
7       0x7f137e84f253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f137e84f253]
8       0x7f137e5deac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f137e5deac3]
9       0x7f137e670850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f137e670850]
[TensorRT-LLM][WARNING] Step function failed, continuing.
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/triton/llm_pipeline/trt_pipeline.py", "line_number": 310, "request_id": "1ddbaf79-8aa8-4b70-a14f-902b0c11b6cb", "time": "2024-04-19 10:11:13,897897", "message": "model inference failed -- Encountered error for requestId 5039179871698361612: Encountered an error in forward function: Input tensor 'attn_qkv_lora_weights_pointers_0' not found; expected shape: (-1, 2) (/app/tensorrt_llm/cpp/tensorrt_llm/runtime/tllmRuntime.cpp:123)
1       0x7f12aa10eb9f tensorrt_llm::runtime::TllmRuntime::setInputTensors(int, std::unordered_map<std::string, std::shared_ptr<tensorrt_llm::runtime::ITensor>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::shared_ptr<tensorrt_llm::runtime::ITensor> > > > const&) + 607
2       0x7f12aa1465de tensorrt_llm::batch_manager::TrtGptModelInflightBatching::setupContext(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > > const&, std::vector<unsigned long, std::allocator<unsigned long> > const&, int, int) + 206
3       0x7f12aa1476f8 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::executeBatch(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > >&) + 1160
4       0x7f12aa14d724 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::forward(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&) + 3716
5       0x7f12aa11da68 tensorrt_llm::batch_manager::GptManager::step(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&) + 56
6       0x7f12aa1227c7 tensorrt_llm::batch_manager::GptManager::decoupled_execution_loop() + 247
7       0x7f137e84f253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f137e84f253]
8       0x7f137e5deac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f137e5deac3]
9       0x7f137e670850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f137e670850]"}
{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/utils.py", "line_number": 25, "request_id": "None", "time": "2024-04-19 10:11:13,899899", "message": "model inference failed -- Encountered error for requestId 5039179871698361612: Encountered an error in forward function: Input tensor 'attn_qkv_lora_weights_pointers_0' not found; expected shape: (-1, 2) (/app/tensorrt_llm/cpp/tensorrt_llm/runtime/tllmRuntime.cpp:123)
1       0x7f12aa10eb9f tensorrt_llm::runtime::TllmRuntime::setInputTensors(int, std::unordered_map<std::string, std::shared_ptr<tensorrt_llm::runtime::ITensor>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::shared_ptr<tensorrt_llm::runtime::ITensor> > > > const&) + 607
2       0x7f12aa1465de tensorrt_llm::batch_manager::TrtGptModelInflightBatching::setupContext(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > > const&, std::vector<unsigned long, std::allocator<unsigned long> > const&, int, int) + 206
3       0x7f12aa1476f8 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::executeBatch(std::map<unsigned long, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > > >&) + 1160
4       0x7f12aa14d724 tensorrt_llm::batch_manager::TrtGptModelInflightBatching::forward(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&) + 3716
5       0x7f12aa11da68 tensorrt_llm::batch_manager::GptManager::step(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&) + 56
6       0x7f12aa1227c7 tensorrt_llm::batch_manager::GptManager::decoupled_execution_loop() + 247
7       0x7f137e84f253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f137e84f253]
8       0x7f137e5deac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f137e5deac3]
9       0x7f137e670850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f137e670850]"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/entrypoints/openai.py", "line_number": 197, "request_id": "None", "time": "2024-04-19 10:11:27,086086", "message": "Received chat completion request"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/triton/llm_pipeline/trt_pipeline.py", "line_number": 246, "request_id": "23c7e29d-7b3e-47b4-aa07-416326dee38b", "time": "2024-04-19 10:11:27,170170", "message": "trt_llm_input_tensors ['input_ids', 'input_lengths', 'request_output_len', 'runtime_top_k', 'runtime_top_p', 'temperature', 'len_penalty', 'repetition_penalty', 'random_seed', 'return_log_probs', 'beam_width', 'end_id', 'stop_words_list', 'bad_words_list']"}
{"level": "INFO", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/triton/llm_pipeline/trt_pipeline.py", "line_number": 274, "request_id": "23c7e29d-7b3e-47b4-aa07-416326dee38b", "time": "2024-04-19 10:11:27,170170", "message": "output_names ['output_ids', 'sequence_length', 'output_log_probs', 'cum_log_probs']"}
Signal (11) received.
 0# 0x0000556B9D9D377D in tritonserver
 1# 0x00007F137E58C520 in /usr/lib/x86_64-linux-gnu/libc.so.6
 2# tensorrt_llm::runtime::GptDecoderBatch::forwardSync(tensorrt_llm::runtime::decoder_batch::Token const&) in /opt/tritonserver/backends/tensorrtllm/libtensorrt_llm.so
 3# tensorrt_llm::batch_manager::TrtGptModelInflightBatching::forward(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&) in /opt/tritonserver/backends/tensorrtllm/libtensorrt_llm.so
 4# tensorrt_llm::batch_manager::GptManager::step(std::list<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest>, std::allocator<std::shared_ptr<tensorrt_llm::batch_manager::LlmRequest> > >&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&) in /opt/tritonserver/backends/tensorrtllm/libtensorrt_llm.so
 5# tensorrt_llm::batch_manager::GptManager::decoupled_execution_loop() in /opt/tritonserver/backends/tensorrtllm/libtensorrt_llm.so
 6# 0x00007F137E84F253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6
 7# 0x00007F137E5DEAC3 in /usr/lib/x86_64-linux-gnu/libc.so.6
 8# 0x00007F137E670850 in /usr/lib/x86_64-linux-gnu/libc.so.6

{"level": "ERROR", "file_path": "/usr/local/lib/python3.10/dist-packages/nemollm_inference/sdk/utils.py", "line_number": 25, "request_id": "None", "time": "2024-04-19 10:11:32,749749", "message": "Socket closed"}
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
I0419 10:11:33.209010 259 pb_stub.cc:1963]  Non-graceful termination detected. 
I0419 10:11:33.379404 260 pb_stub.cc:1963]  Non-graceful termination detected. 
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node is-nim-predictor-00001-deployment-cf9cc678f-jl2sv exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Triton command return code: {triton_return_code}